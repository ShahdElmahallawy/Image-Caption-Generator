# Image Caption Generator

## Overview
This Image Caption Generator project enhances an existing attention-based Encoder-Decoder model by Al-Malla et al., adapting it for the Flickr30k dataset and introducing architectural improvements for better accuracy. Our goal is to aid visually impaired individuals by generating descriptive captions from images.

## Key Features
- **Customized Encoder-Decoder Architecture:** Modified from the original model by Al-Malla et al. to include additional layers and activation functions.
- **Data Augmentation:** Utilized augmented datasets to improve model robustness against diverse image inputs.

## Contributors
- Shahd Elmahallawy
- Nermien Elassy

## Acknowledgements
- This project is based on the model proposed by Muhammad Abdelhadie Alâ€‘Malla, Assef Jafar, and Nada Ghneim, detailed in their paper on attention and object features for image captioning. The original paper can be accessed here.
- We utilized the Flickr30k dataset, which significantly differs from the datasets used in the original study, introducing unique challenges and insights.
